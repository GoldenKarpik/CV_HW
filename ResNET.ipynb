{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NKZki6B9BUqC"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from collections import Counter, defaultdict\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import precision_recall_fscore_support, confusion_matrix, classification_report\n",
        "from sklearn.model_selection import train_test_split\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms, models\n",
        "from tqdm import tqdm\n",
        "from PIL import Image\n",
        "\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "random.seed(42)\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class TinyImageNetDataset(Dataset):\n",
        "    def __init__(self, root_dir, class_names=None, split='train', transform=None):\n",
        "        self.root_dir = root_dir\n",
        "        self.split = split\n",
        "        self.transform = transform\n",
        "\n",
        "        # загружаем все классы\n",
        "        with open(os.path.join(root_dir, 'wnids.txt'), 'r') as f:\n",
        "            all_class_names = [line.strip() for line in f]\n",
        "\n",
        "        # выбираем подмножество классов\n",
        "        if class_names is None:\n",
        "            class_names = all_class_names\n",
        "        self.class_names = class_names\n",
        "\n",
        "        # создаем маппинг имен в индексы\n",
        "        self.class_to_idx = {name: i for i, name in enumerate(self.class_names)}\n",
        "\n",
        "        # загружаем образцы\n",
        "        self.samples = self._make_dataset()\n",
        "\n",
        "        print(f\"Создан датасет {split}: {len(self.samples)} образцов, {len(self.class_names)} классов\")\n",
        "\n",
        "    def _make_dataset(self):\n",
        "        data = []\n",
        "\n",
        "        if self.split == 'train':\n",
        "            train_dir = os.path.join(self.root_dir, 'train')\n",
        "            for cls_name in self.class_names:\n",
        "                img_dir = os.path.join(train_dir, cls_name, 'images')\n",
        "                if not os.path.exists(img_dir):\n",
        "                    continue\n",
        "                for img_name in os.listdir(img_dir):\n",
        "                    img_path = os.path.join(img_dir, img_name)\n",
        "                    label = self.class_to_idx[cls_name]\n",
        "                    data.append((img_path, label))\n",
        "\n",
        "        elif self.split == 'val':\n",
        "            val_dir = os.path.join(self.root_dir, 'val')\n",
        "            img_dir = os.path.join(val_dir, 'images')\n",
        "            anno_path = os.path.join(val_dir, 'val_annotations.txt')\n",
        "\n",
        "            # создаем маппинг изображение-класс\n",
        "            label_map = {}\n",
        "            with open(anno_path, 'r') as f:\n",
        "                for line in f:\n",
        "                    img_name, cls_name, *_ = line.strip().split('\\t')\n",
        "                    if cls_name in self.class_names:\n",
        "                        label_map[img_name] = self.class_to_idx[cls_name]\n",
        "\n",
        "            for img_name in os.listdir(img_dir):\n",
        "                if img_name in label_map:\n",
        "                    img_path = os.path.join(img_dir, img_name)\n",
        "                    data.append((img_path, label_map[img_name]))\n",
        "\n",
        "        elif self.split == 'test':\n",
        "            test_dir = os.path.join(self.root_dir, 'test', 'images')\n",
        "            for img_name in os.listdir(test_dir):\n",
        "                img_path = os.path.join(test_dir, img_name)\n",
        "                data.append((img_path, -1))  # тест без меток\n",
        "\n",
        "        return data\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path, label = self.samples[idx]\n",
        "        img = Image.open(img_path).convert('RGB')\n",
        "        if self.transform:\n",
        "            img = self.transform(img)\n",
        "        return img, label\n",
        "\n",
        "\n",
        "def denormalize(img_tensor):\n",
        "    # изображение из нормализовнного состояния\n",
        "    mean = torch.tensor([0.485, 0.456, 0.406]).view(1, 1, 3)\n",
        "    std = torch.tensor([0.229, 0.224, 0.225]).view(1, 1, 3)\n",
        "    img = img_tensor.permute(1, 2, 0) * std + mean\n",
        "    return img.clamp(0, 1)"
      ],
      "metadata": {
        "id": "J3FjCEf1Cysx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BasicBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, stride=1, downsample=None, activation='relu'):\n",
        "        super().__init__()\n",
        "\n",
        "        # сохраняем тип активации для использования в forward\n",
        "        self.activation_type = activation\n",
        "        self.activation = self._get_activation(activation)\n",
        "\n",
        "        # первый сверточный слой\n",
        "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3,\n",
        "                               padding=1, stride=stride, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
        "\n",
        "        # второй сверточный слой\n",
        "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3,\n",
        "                               padding=1, stride=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
        "\n",
        "        # skip connection если вдруг размерносить не совпадут\n",
        "        self.downsample = downsample\n",
        "\n",
        "    def _get_activation(self, activation):\n",
        "        if activation == 'relu':\n",
        "            return nn.ReLU(inplace=True)\n",
        "        elif activation == 'leaky_relu':\n",
        "            return nn.LeakyReLU(negative_slope=0.01, inplace=True)\n",
        "        elif activation == 'elu':\n",
        "            return nn.ELU(alpha=1.0, inplace=True)\n",
        "        elif activation == 'gelu':\n",
        "            return nn.GELU()\n",
        "        else:\n",
        "            return nn.ReLU(inplace=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # сохраняем вход для skip connection\n",
        "        identity = x\n",
        "\n",
        "        # первый сверток + батч-норм + активация\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.activation(out)\n",
        "\n",
        "        # второй сверток + батч-норм\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "\n",
        "        # skip connection\n",
        "        if self.downsample is not None:\n",
        "            identity = self.downsample(x)\n",
        "\n",
        "        # складываем и применяем активацию\n",
        "        out += identity\n",
        "        out = self.activation(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class ResNet18(nn.Module):\n",
        "    def __init__(self, num_classes=10, channels=None, num_blocks=None, activation='relu'):\n",
        "        super().__init__()\n",
        "\n",
        "        if channels is None:\n",
        "            channels = [64, 128, 256, 512]\n",
        "        if num_blocks is None:\n",
        "            num_blocks = [2, 2, 2, 2]\n",
        "\n",
        "        # проверка длины\n",
        "        assert len(channels) == len(num_blocks)\n",
        "\n",
        "        self.channels = channels\n",
        "        self.num_blocks = num_blocks\n",
        "\n",
        "        self.conv1 = nn.Conv2d(3, channels[0], kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(channels[0])\n",
        "        self.activation = self._get_activation(activation)\n",
        "\n",
        "        # residual слои\n",
        "        self.layers = nn.ModuleList()\n",
        "        in_channels = channels[0]\n",
        "\n",
        "        for i, (out_channels, num_block) in enumerate(zip(channels, num_blocks)):\n",
        "            # для первого слоя stride=1, для остальных stride=2\n",
        "            stride = 1 if i == 0 else 2\n",
        "            layer = self._make_layer(in_channels, out_channels, num_block, stride, activation)\n",
        "            self.layers.append(layer)\n",
        "            in_channels = out_channels\n",
        "\n",
        "        # адаптивный pooling и классификатор\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.fc = nn.Linear(channels[-1], num_classes)\n",
        "        self._initialize_weights()\n",
        "\n",
        "    def _get_activation(self, activation):\n",
        "        if activation == 'relu':\n",
        "            return nn.ReLU(inplace=True)\n",
        "        elif activation == 'leaky_relu':\n",
        "            return nn.LeakyReLU(negative_slope=0.01, inplace=True)\n",
        "        elif activation == 'elu':\n",
        "            return nn.ELU(alpha=1.0, inplace=True)\n",
        "        elif activation == 'gelu':\n",
        "            return nn.GELU()\n",
        "        else:\n",
        "            return nn.ReLU(inplace=True)\n",
        "\n",
        "    def _make_layer(self, in_channels, out_channels, num_blocks, stride, activation):\n",
        "        layers = []\n",
        "\n",
        "        # первый блок может иметь downsample и stride=2\n",
        "        downsample = None\n",
        "        if stride != 1 or in_channels != out_channels:\n",
        "            downsample = nn.Sequential(\n",
        "                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(out_channels)\n",
        "            )\n",
        "\n",
        "        layers.append(BasicBlock(in_channels, out_channels, stride, downsample, activation))\n",
        "\n",
        "        # остальные блоки (stride=1, без downsample)\n",
        "        for _ in range(1, num_blocks):\n",
        "            layers.append(BasicBlock(out_channels, out_channels, activation=activation))\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def _initialize_weights(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                nn.init.constant_(m.weight, 1)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.activation(x)\n",
        "        # x = self.maxpool(x)\n",
        "\n",
        "        # residual блоки\n",
        "        for layer in self.layers:\n",
        "            x = layer(x)\n",
        "\n",
        "        # классификация\n",
        "        x = self.avgpool(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.fc(x)\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "pubBiJKVC8_J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_epoch(model, loader, optimizer, criterion, device):\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    pbar = tqdm(loader, desc=\"Обучение\")\n",
        "    for data, target in pbar:\n",
        "        data, target = data.to(device), target.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = criterion(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        correct += (output.argmax(1) == target).sum().item()\n",
        "        total += target.size(0)\n",
        "\n",
        "        pbar.set_postfix({\n",
        "            'loss': f'{total_loss / total:.4f}',\n",
        "            'acc': f'{100. * correct / total:.2f}%'\n",
        "        })\n",
        "\n",
        "    return total_loss / len(loader), 100. * correct / total\n",
        "\n",
        "\n",
        "def validate_epoch(model, loader, criterion, device):\n",
        "    model.eval()\n",
        "    total_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    pbar = tqdm(loader, desc=\"Валидация\")\n",
        "    with torch.no_grad():\n",
        "        for data, target in pbar:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            loss = criterion(output, target)\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            correct += (output.argmax(1) == target).sum().item()\n",
        "            total += target.size(0)\n",
        "\n",
        "            pbar.set_postfix({\n",
        "                'loss': f'{total_loss / total:.4f}',\n",
        "                'acc': f'{100. * correct / total:.2f}%'\n",
        "            })\n",
        "\n",
        "    return total_loss / len(loader), 100. * correct / total\n",
        "\n",
        "\n",
        "def train_model(model, train_loader, val_loader, optimizer, criterion, device, num_epochs=20):\n",
        "    print(f\"\\n {num_epochs} эпох\\n\")\n",
        "\n",
        "    history = {\n",
        "        'train_loss': [], 'train_acc': [],\n",
        "        'val_loss': [], 'val_acc': []\n",
        "    }\n",
        "    best_val_acc = 0.0\n",
        "\n",
        "    model = model.to(device)\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print(f\"Эпоха {epoch + 1}/{num_epochs}\")\n",
        "\n",
        "        train_loss, train_acc = train_epoch(model, train_loader, optimizer, criterion, device)\n",
        "        val_loss, val_acc = validate_epoch(model, val_loader, criterion, device)\n",
        "\n",
        "        history['train_loss'].append(train_loss)\n",
        "        history['train_acc'].append(train_acc)\n",
        "        history['val_loss'].append(val_loss)\n",
        "        history['val_acc'].append(val_acc)\n",
        "\n",
        "        print(f\"  Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%\")\n",
        "        print(f\"  Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%\\n\")\n",
        "\n",
        "        if val_acc > best_val_acc:\n",
        "            best_val_acc = val_acc\n",
        "            torch.save(model.state_dict(), 'best_model.pth')\n",
        "            print(f\"  лучшая модель: {val_acc:.2f}%\\n\")\n",
        "\n",
        "    return history\n",
        "\n",
        "\n",
        "def plot_training_history(history):\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "    # график потерь\n",
        "    ax1.plot(history['train_loss'], label='Train Loss', color='blue', linewidth=2)\n",
        "    ax1.plot(history['val_loss'], label='Val Loss', color='red', linewidth=2)\n",
        "    ax1.set_title('Потери')\n",
        "    ax1.set_xlabel('Эпоха')\n",
        "    ax1.set_ylabel('Loss')\n",
        "    ax1.legend()\n",
        "    ax1.grid(True, alpha=0.3)\n",
        "\n",
        "    # график точности\n",
        "    ax2.plot(history['train_acc'], label='Train Acc', color='blue', linewidth=2)\n",
        "    ax2.plot(history['val_acc'], label='Val Acc', color='red', linewidth=2)\n",
        "    ax2.set_title('Точность')\n",
        "    ax2.set_xlabel('Эпоха')\n",
        "    ax2.set_ylabel('Accuracy (%)')\n",
        "    ax2.legend()\n",
        "    ax2.grid(True, alpha=0.3)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "\n",
        "def run_experiment(model, num_epochs=20, model_name='model'):\n",
        "    # считаем параметры\n",
        "    params = count_parameters(model)\n",
        "    print(f\"Параметры: {params:,}\\n\")\n",
        "\n",
        "    # вывод модели\n",
        "    print(model)\n",
        "\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    history = train_model(model, train_loader, val_loader, optimizer, criterion, device, num_epochs)\n",
        "\n",
        "    # сохраняем модель\n",
        "    torch.save(model.state_dict(), f'{model_name}.pth')\n",
        "    print(f\"модель сохранена: {model_name}.pth\\n\")\n",
        "\n",
        "    # график\n",
        "    plot_training_history(history)\n",
        "\n",
        "    return {\n",
        "        'params': params,\n",
        "        'train_acc': history['train_acc'][-1],\n",
        "        'val_acc': history['val_acc'][-1],\n",
        "        'history': history\n",
        "    }\n",
        "\n",
        "\n",
        "def visualize_predictions(model, loader, num_samples=10, title=\"\"):\n",
        "    model.eval()\n",
        "    plt.figure(figsize=(15, 8))\n",
        "    images_shown = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for data, targets in loader:\n",
        "            data = data.to(device)\n",
        "            outputs = model(data)\n",
        "            _, predictions = torch.max(outputs, 1)\n",
        "\n",
        "            for i in range(data.size(0)):\n",
        "                if images_shown >= num_samples or targets[i] == -1:\n",
        "                    continue\n",
        "\n",
        "                plt.subplot(2, 5, images_shown + 1)\n",
        "                img_vis = denormalize(data[i].cpu())\n",
        "                plt.imshow(img_vis)\n",
        "\n",
        "                true_label = targets[i].item()\n",
        "                pred_label = predictions[i].item()\n",
        "                color = 'green' if true_label == pred_label else 'red'\n",
        "\n",
        "                plt.title(f\"Истинный: {true_label}\\nПрдскзнный: {pred_label}\", color=color)\n",
        "                plt.axis('off')\n",
        "                images_shown += 1\n",
        "\n",
        "            if images_shown >= num_samples:\n",
        "                break\n",
        "\n",
        "    plt.suptitle(title, fontsize=16)\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "fmqIDrp6DKWp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "root_dir = \"/content/drive/MyDrive/customREZNET/tiny-imagenet-200\"\n",
        "\n",
        "with open(os.path.join(root_dir, 'wnids.txt'), 'r') as f:\n",
        "    all_class_names = [line.strip() for line in f]\n",
        "\n",
        "# выбираем 10 классов (можно выбрать любые, но мне лень, берем первые 10)\n",
        "selected_classes = all_class_names[:10]\n",
        "print(f\"Выбранные классы:{selected_classes}\")\n",
        "\n",
        "# проверяем, сколько изображений у нас есть для этих классов\n",
        "train_samples = []\n",
        "for cls in selected_classes:\n",
        "    img_dir = os.path.join(root_dir, 'train', cls, 'images')\n",
        "    if os.path.exists(img_dir):\n",
        "        count = len(os.listdir(img_dir))\n",
        "        train_samples.append((cls, count))\n",
        "        print(f\"Класс {cls}: {count} изображений\")\n",
        "\n",
        "# Трансформации для обучения и валидации\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.Resize((72, 72)),\n",
        "    transforms.RandomResizedCrop(64, scale=(0.8, 1.0)),\n",
        "    transforms.RandomHorizontalFlip(p=0.5),\n",
        "    transforms.RandomRotation(15),\n",
        "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "val_transform = transforms.Compose([\n",
        "    transforms.Resize((64, 64)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "# Создаем датасеты\n",
        "train_dataset = TinyImageNetDataset(root_dir, selected_classes, split='train', transform=train_transform)\n",
        "val_dataset = TinyImageNetDataset(root_dir, selected_classes, split='val', transform=val_transform)\n",
        "test_dataset = TinyImageNetDataset(root_dir, selected_classes, split='test', transform=val_transform)\n",
        "\n",
        "print(f\"\\ntrain: {len(train_dataset)}\")\n",
        "print(f\"val: {len(val_dataset)}\")\n",
        "print(f\"test: {len(test_dataset)}\")\n",
        "\n",
        "# Создаем DataLoader'ы\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=0, pin_memory=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False, num_workers=0, pin_memory=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False, num_workers=0, pin_memory=True)\n",
        "\n",
        "# Показываем несколько примеров из обучающего набора\n",
        "images, labels = next(iter(train_loader))\n",
        "\n",
        "plt.figure(figsize=(15, 6))\n",
        "for i in range(10):\n",
        "    plt.subplot(2, 5, i + 1)\n",
        "    img_vis = denormalize(images[i])\n",
        "    plt.imshow(img_vis)\n",
        "    plt.title(f\"Class: {labels[i].item()}\")\n",
        "    plt.axis('off')\n",
        "plt.suptitle(\"Примеры изображений из выбранных 10 классов\", fontsize=16)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f\"размер батча: {images.shape}\")\n",
        "print(f\"диапазон значений: [{images.min():.3f}, {images.max():.3f}]\")"
      ],
      "metadata": {
        "id": "Rmb-13roDOCp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3.1\n",
        "baseline_model = ResNet18(\n",
        "    num_classes=10,\n",
        "    channels=[64, 128, 256],\n",
        "    num_blocks=[2, 2, 2],\n",
        "    activation='relu'\n",
        ").to(device)\n",
        "\n",
        "baseline_result = run_experiment(baseline_model, num_epochs=20, model_name='baseline_model')\n",
        "\n",
        "# 3.2-A\n",
        "model_blocks_A = ResNet18(\n",
        "    num_classes=10,\n",
        "    channels=[32, 64, 128, 256],\n",
        "    num_blocks=[1, 1, 1, 1],\n",
        "    activation='relu'\n",
        ").to(device)\n",
        "result_blocks_A = run_experiment(model_blocks_A, num_epochs=20, model_name='model_blocks_A')\n",
        "\n",
        "# 3.2-B\n",
        "model_blocks_B = ResNet18(\n",
        "    num_classes=10,\n",
        "    channels=[32, 64, 128, 256],\n",
        "    num_blocks=[2, 2, 2, 2],\n",
        "    activation='relu'\n",
        ").to(device)\n",
        "result_blocks_B = run_experiment(model_blocks_B, num_epochs=20, model_name='model_blocks_B')\n",
        "\n",
        "# 3.2-C\n",
        "model_blocks_C = ResNet18(\n",
        "    num_classes=10,\n",
        "    channels=[32, 64, 128, 256],\n",
        "    num_blocks=[3, 3, 3, 3],\n",
        "    activation='relu'\n",
        ").to(device)\n",
        "result_blocks_C = run_experiment(model_blocks_C, num_epochs=20, model_name='model_blocks_C')\n",
        "\n",
        "# 4\n",
        "activations = {\n",
        "    '3.3-A': ('ReLU', 'relu'),\n",
        "    '3.3-B': ('LeakyReLU', 'leaky_relu'),\n",
        "    '3.3-C': ('ELU', 'elu'),\n",
        "    '3.3-D': ('GELU', 'gelu'),\n",
        "}\n",
        "\n",
        "activation_results = {}\n",
        "\n",
        "for exp_key, (name, act_func) in activations.items():\n",
        "    model_act = ResNet18(\n",
        "        num_classes=10,\n",
        "        channels=[32, 64, 128, 256],\n",
        "        num_blocks=[2, 2, 2, 2],\n",
        "        activation=act_func\n",
        "    ).to(device)\n",
        "\n",
        "    result_act = run_experiment(model_act, num_epochs=20, model_name=f'model_{exp_key}')\n",
        "    activation_results[exp_key] = result_act"
      ],
      "metadata": {
        "id": "1qckYc5uDc5p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_model = ResNet18(\n",
        "    num_classes=10,\n",
        "    channels=[32, 64, 128, 256],\n",
        "    num_blocks=[2, 2, 2, 2],\n",
        "    activation='leaky_relu'\n",
        ").to(device)\n",
        "\n",
        "final_result = run_experiment(final_model, num_epochs=35, model_name='final_model')\n",
        "\n",
        "# тестирование на test set\n",
        "final_model.load_state_dict(torch.load('best_model.pth'))\n",
        "final_model.eval()\n",
        "\n",
        "all_preds, all_labels = [], []\n",
        "\n",
        "with torch.no_grad():\n",
        "    with tqdm(test_loader) as pbar:\n",
        "        for data, target in pbar:\n",
        "            data = data.to(device)\n",
        "            output = final_model(data)\n",
        "            _, predicted = torch.max(output, 1)\n",
        "            all_preds.extend(predicted.cpu().numpy())\n",
        "            all_labels.extend(target.numpy())\n",
        "\n",
        "valid_indices = [i for i, label in enumerate(all_labels) if label != -1]\n",
        "if valid_indices:\n",
        "    test_preds = [all_preds[i] for i in valid_indices]\n",
        "    test_labels = [all_labels[i] for i in valid_indices]\n",
        "\n",
        "    print(f\"\\nTest Accuracy: {100 * sum(p == l for p, l in zip(test_preds, test_labels)) / len(test_labels):.2f}%\")\n",
        "    print(classification_report(test_labels, test_preds, target_names=[f\"Class_{i}\" for i in range(10)]))\n",
        "\n",
        "    # confusion matrix\n",
        "    cm = confusion_matrix(test_labels, test_preds)\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "                xticklabels=[f\"Class_{i}\" for i in range(10)],\n",
        "                yticklabels=[f\"Class_{i}\" for i in range(10)])\n",
        "    plt.title('Confusion Matrix')\n",
        "    plt.ylabel('истинный класс')\n",
        "    plt.xlabel('предсказанный класс')\n",
        "    plt.show()\n",
        "\n",
        "# визуализация с исправленными подписями\n",
        "visualize_predictions(final_model, test_loader, title=\"Задание 5: Финальная модель LeakyReLU\")"
      ],
      "metadata": {
        "id": "EVUkJ-WgDiGI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_results_data = [\n",
        "    {\n",
        "        'Этап': 'Baseline',\n",
        "        'Конфигурация': '[64,128,256]',\n",
        "        'Параметры': f\"{baseline_result['params']:,}\",\n",
        "        'Val Accuracy': f\"{baseline_result['val_acc']:.2f}%\",\n",
        "        'Train Accuracy': f\"{baseline_result['train_acc']:.2f}%\"\n",
        "    },\n",
        "    {\n",
        "        'Этап': '3.2-A',\n",
        "        'Конфигурация': '[1,1,1,1] блоков',\n",
        "        'Параметры': f\"{result_blocks_A['params']:,}\",\n",
        "        'Val Accuracy': f\"{result_blocks_A['val_acc']:.2f}%\",\n",
        "        'Train Accuracy': f\"{result_blocks_A['train_acc']:.2f}%\"\n",
        "    },\n",
        "    {\n",
        "        'Этап': '3.2-B',\n",
        "        'Конфигурация': '[2,2,2,2] блоков',\n",
        "        'Параметры': f\"{result_blocks_B['params']:,}\",\n",
        "        'Val Accuracy': f\"{result_blocks_B['val_acc']:.2f}%\",\n",
        "        'Train Accuracy': f\"{result_blocks_B['train_acc']:.2f}%\"\n",
        "    },\n",
        "    {\n",
        "        'Этап': '3.2-C',\n",
        "        'Конфигурация': '[3,3,3,3] блоков',\n",
        "        'Параметры': f\"{result_blocks_C['params']:,}\",\n",
        "        'Val Accuracy': f\"{result_blocks_C['val_acc']:.2f}%\",\n",
        "        'Train Accuracy': f\"{result_blocks_C['train_acc']:.2f}%\"\n",
        "    },\n",
        "]\n",
        "\n",
        "for exp_key, result in activation_results.items():\n",
        "    name = activations[exp_key][0]\n",
        "    all_results_data.append({\n",
        "        'Этап': exp_key,\n",
        "        'Конфигурация': name,\n",
        "        'Параметры': f\"{result['params']:,}\",\n",
        "        'Val Accuracy': f\"{result['val_acc']:.2f}%\",\n",
        "        'Train Accuracy': f\"{result['train_acc']:.2f}%\"\n",
        "    })\n",
        "\n",
        "all_results_data.append({\n",
        "    'Этап': 'Final',\n",
        "    'Конфигурация': '32 64 128 256 + LeakyReLU',\n",
        "    'Параметры': f\"{final_result['params']:,}\",\n",
        "    'Val Accuracy': f\"{final_result['val_acc']:.2f}%\",\n",
        "    'Train Accuracy': f\"{final_result['train_acc']:.2f}%\"\n",
        "})\n",
        "\n",
        "df_results = pd.DataFrame(all_results_data)\n",
        "df_results = df_results.sort_values('Val Accuracy', ascending=False)\n",
        "\n",
        "print(\"\\nИтоговая таблица всех экспериментов:\")\n",
        "print(df_results.to_string(index=False))\n",
        "\n",
        "df_results.to_csv('experiment_results.csv', index=False)"
      ],
      "metadata": {
        "id": "HqH7MZ7qD58R"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}